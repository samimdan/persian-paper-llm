{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863a6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "doc=Document('thesis1.docx')\n",
    "text=\" \".join([p.text for p in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78af307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "cleaned_text = clean_text(text)\n",
    "sentences = cleaned_text.split(\".\") \n",
    "words = cleaned_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfdac1",
   "metadata": {},
   "source": [
    "preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "X = []\n",
    "y = []\n",
    "for i in range(window_size, len(words)):\n",
    "    X.append(words[i-window_size:i])\n",
    "    y.append(words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aaae6d",
   "metadata": {},
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f339be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter(words)\n",
    "vocab = [\"<pad>\", \"<unk>\"] + list(counter.keys())\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "X_idx = [\n",
    "    [word2idx.get(w, word2idx[\"<unk>\"]) for w in seq]\n",
    "    for seq in X\n",
    "]\n",
    "\n",
    "y_idx = [word2idx.get(w, word2idx[\"<unk>\"]) for w in y]\n",
    "\n",
    "X = torch.tensor(X_idx)\n",
    "y = torch.tensor(y_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73418b",
   "metadata": {},
   "source": [
    "convert each word to unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4afba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fc03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91b3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d681d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.cuda()\n",
    "y=y.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f65db73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_35136\\2279556815.py:2: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  fasttext_model = FastText.load_fasttext_format('cc.fa.300.bin')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText.load_fasttext_format('cc.fa.300.bin')\n",
    "embed_dim = 300\n",
    "embedding_matrix = torch.zeros((vocab_size, embed_dim))\n",
    "for word, idx in word2idx.items():\n",
    "  if word in fasttext_model.wv:\n",
    "    embedding_matrix[idx] = torch.tensor(fasttext_model.wv[word])\n",
    "    embedding_matrix=embedding_matrix.cuda()\n",
    "  else:\n",
    "    embedding_matrix[idx] = torch.randn(embed_dim)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ffaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "embedding_layer=nn.Embedding.from_pretrained(embedding_matrix,freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NextWordBiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True \n",
    "        )\n",
    "\n",
    "        # مقدار embedding_dim رو از nn.Embedding بدست میاریم\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "    \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "     \n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (B, T, embedding_dim)\n",
    "        out, _ = self.bilstm(x)  # (B, T, 2*hidden_dim)\n",
    "        out = out[:, -1, :]  \n",
    "        out = self.fc(out)  # (B, vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44732f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_35136\\3447138470.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(embedding_matrix, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.497747421264648\n",
      "Epoch 2, Loss: 7.4865899085998535\n",
      "Epoch 3, Loss: 7.475114345550537\n",
      "Epoch 4, Loss: 7.462834358215332\n",
      "Epoch 5, Loss: 7.449239253997803\n",
      "Epoch 6, Loss: 7.433797836303711\n",
      "Epoch 7, Loss: 7.415948867797852\n",
      "Epoch 8, Loss: 7.395096778869629\n",
      "Epoch 9, Loss: 7.3705644607543945\n",
      "Epoch 10, Loss: 7.341572284698486\n",
      "Epoch 11, Loss: 7.307217121124268\n",
      "Epoch 12, Loss: 7.266456604003906\n",
      "Epoch 13, Loss: 7.21812105178833\n",
      "Epoch 14, Loss: 7.1609649658203125\n",
      "Epoch 15, Loss: 7.093814373016357\n",
      "Epoch 16, Loss: 7.015848159790039\n",
      "Epoch 17, Loss: 6.927062034606934\n",
      "Epoch 18, Loss: 6.828914642333984\n",
      "Epoch 19, Loss: 6.724972248077393\n",
      "Epoch 20, Loss: 6.621257781982422\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NextWordBiLSTM(\n",
    "    embedding_matrix,\n",
    "    hidden_dim=128,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "def sample_top_k(output, top_k=5, temperature=0.8):\n",
    "\n",
    "    logits = output.squeeze() / temperature\n",
    "    probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "\n",
    "    top_indices = probs.argsort()[-top_k:]\n",
    "    top_probs = probs[top_indices]\n",
    "    top_probs = top_probs / top_probs.sum()  # نرمال‌سازی\n",
    "    chosen_idx = np.random.choice(top_indices, p=top_probs)\n",
    "    \n",
    "    return chosen_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a32fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_text, next_words=50, seq_length=10,\n",
    "                  top_k=5, temperature=0.8, device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "    words = seed_text.split()\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # استفاده از آخرین seq_length کلمه\n",
    "        input_seq = [word2idx.get(w, word2idx[\"<unk>\"]) for w in words[-seq_length:]]\n",
    "        \n",
    "        # اگر seed کوتاه بود، با <pad> پر می‌کنیم\n",
    "        if len(input_seq) < seq_length:\n",
    "            input_seq = [word2idx[\"<pad>\"]] * (seq_length - len(input_seq)) + input_seq\n",
    "        \n",
    "        input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            predicted_idx = sample_top_k(output, top_k=top_k, temperature=temperature)\n",
    "            predicted_word = idx2word.get(predicted_idx, \"<unk>\")\n",
    "\n",
    "\n",
    "        if len(words) > 1 and predicted_word == words[-1]:\n",
    "            continue  \n",
    "        \n",
    "        words.append(predicted_word)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "seed = \"تحلیل و شناسایی الگوهای رفتاری\"\n",
    "text = generate_text(model, seed_text=seed, next_words=30, \n",
    "                     seq_length=10, top_k=5, temperature=0.8, device=device)\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
