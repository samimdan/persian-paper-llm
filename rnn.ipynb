{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863a6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "doc=Document('thesis1.docx')\n",
    "text=\" \".join([p.text for p in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78af307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "cleaned_text = clean_text(text)\n",
    "sentences = cleaned_text.split(\".\") \n",
    "words = cleaned_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfdac1",
   "metadata": {},
   "source": [
    "preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "X = []\n",
    "y = []\n",
    "for i in range(window_size, len(words)):\n",
    "    X.append(words[i-window_size:i])\n",
    "    y.append(words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aaae6d",
   "metadata": {},
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f339be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter(words)\n",
    "vocab = [\"<pad>\", \"<unk>\"] + list(counter.keys())\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "X_idx = [\n",
    "    [word2idx.get(w, word2idx[\"<unk>\"]) for w in seq]\n",
    "    for seq in X\n",
    "]\n",
    "\n",
    "y_idx = [word2idx.get(w, word2idx[\"<unk>\"]) for w in y]\n",
    "\n",
    "X = torch.tensor(X_idx)\n",
    "y = torch.tensor(y_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73418b",
   "metadata": {},
   "source": [
    "convert each word to unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4afba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fc03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91b3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d681d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65db73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_65672\\2279556815.py:2: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  fasttext_model = FastText.load_fasttext_format('cc.fa.300.bin')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText.load_fasttext_format('cc.fa.300.bin')\n",
    "embed_dim = 300\n",
    "embedding_matrix = torch.zeros((vocab_size, embed_dim))\n",
    "for word, idx in word2idx.items():\n",
    "  if word in fasttext_model.wv:\n",
    "    embedding_matrix[idx] = torch.tensor(fasttext_model.wv[word])\n",
    "    embedding_matrix=embedding_matrix.cuda()\n",
    "  else:\n",
    "    embedding_matrix[idx] = torch.randn(embed_dim)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ffaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "embedding_layer=nn.Embedding.from_pretrained(embedding_matrix,freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2702578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NextWordBiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True \n",
    "        )\n",
    "\n",
    "        # مقدار embedding_dim رو از nn.Embedding بدست میاریم\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "    \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "     \n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (B, T, embedding_dim)\n",
    "        out, _ = self.bilstm(x)  # (B, T, 2*hidden_dim)\n",
    "        out = out[:, -1, :]  \n",
    "        out = self.fc(out)  # (B, vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce1159e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b047974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_65672\\4200312657.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(embedding_matrix, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.506199359893799\n",
      "Epoch 2, Loss: 7.49526834487915\n",
      "Epoch 3, Loss: 7.48415470123291\n",
      "Epoch 4, Loss: 7.472367763519287\n",
      "Epoch 5, Loss: 7.459405899047852\n",
      "Epoch 6, Loss: 7.4447407722473145\n",
      "Epoch 7, Loss: 7.427811145782471\n",
      "Epoch 8, Loss: 7.408025741577148\n",
      "Epoch 9, Loss: 7.384722709655762\n",
      "Epoch 10, Loss: 7.357141017913818\n",
      "Epoch 11, Loss: 7.32439661026001\n",
      "Epoch 12, Loss: 7.2854390144348145\n",
      "Epoch 13, Loss: 7.239078521728516\n",
      "Epoch 14, Loss: 7.184009552001953\n",
      "Epoch 15, Loss: 7.118934631347656\n",
      "Epoch 16, Loss: 7.042820930480957\n",
      "Epoch 17, Loss: 6.955321788787842\n",
      "Epoch 18, Loss: 6.857435703277588\n",
      "Epoch 19, Loss: 6.752220153808594\n",
      "Epoch 20, Loss: 6.645283222198486\n",
      "Epoch 21, Loss: 6.5445170402526855\n",
      "Epoch 22, Loss: 6.458321571350098\n",
      "Epoch 23, Loss: 6.392480850219727\n",
      "Epoch 24, Loss: 6.347497940063477\n",
      "Epoch 25, Loss: 6.3181891441345215\n",
      "Epoch 26, Loss: 6.296279430389404\n",
      "Epoch 27, Loss: 6.275254249572754\n",
      "Epoch 28, Loss: 6.252584934234619\n",
      "Epoch 29, Loss: 6.228478908538818\n",
      "Epoch 30, Loss: 6.2040886878967285\n",
      "Epoch 31, Loss: 6.180568218231201\n",
      "Epoch 32, Loss: 6.158802509307861\n",
      "Epoch 33, Loss: 6.13939905166626\n",
      "Epoch 34, Loss: 6.122734069824219\n",
      "Epoch 35, Loss: 6.10897159576416\n",
      "Epoch 36, Loss: 6.098082065582275\n",
      "Epoch 37, Loss: 6.089870452880859\n",
      "Epoch 38, Loss: 6.083981513977051\n",
      "Epoch 39, Loss: 6.0799078941345215\n",
      "Epoch 40, Loss: 6.077025413513184\n",
      "Epoch 41, Loss: 6.074649810791016\n",
      "Epoch 42, Loss: 6.072123050689697\n",
      "Epoch 43, Loss: 6.068896293640137\n",
      "Epoch 44, Loss: 6.064600944519043\n",
      "Epoch 45, Loss: 6.059078216552734\n",
      "Epoch 46, Loss: 6.052362442016602\n",
      "Epoch 47, Loss: 6.044651508331299\n",
      "Epoch 48, Loss: 6.036235809326172\n",
      "Epoch 49, Loss: 6.027444362640381\n",
      "Epoch 50, Loss: 6.018574237823486\n",
      "Epoch 51, Loss: 6.009851455688477\n",
      "Epoch 52, Loss: 6.00140905380249\n",
      "Epoch 53, Loss: 5.993281364440918\n",
      "Epoch 54, Loss: 5.985419273376465\n",
      "Epoch 55, Loss: 5.9777092933654785\n",
      "Epoch 56, Loss: 5.9700026512146\n",
      "Epoch 57, Loss: 5.96218204498291\n",
      "Epoch 58, Loss: 5.954189777374268\n",
      "Epoch 59, Loss: 5.94602632522583\n",
      "Epoch 60, Loss: 5.937716007232666\n",
      "Epoch 61, Loss: 5.9292988777160645\n",
      "Epoch 62, Loss: 5.920802593231201\n",
      "Epoch 63, Loss: 5.912229061126709\n",
      "Epoch 64, Loss: 5.903532028198242\n",
      "Epoch 65, Loss: 5.894675254821777\n",
      "Epoch 66, Loss: 5.885635852813721\n",
      "Epoch 67, Loss: 5.87642765045166\n",
      "Epoch 68, Loss: 5.867081165313721\n",
      "Epoch 69, Loss: 5.857624530792236\n",
      "Epoch 70, Loss: 5.848066806793213\n",
      "Epoch 71, Loss: 5.838407516479492\n",
      "Epoch 72, Loss: 5.82866096496582\n",
      "Epoch 73, Loss: 5.818860054016113\n",
      "Epoch 74, Loss: 5.8090500831604\n",
      "Epoch 75, Loss: 5.799270153045654\n",
      "Epoch 76, Loss: 5.78953218460083\n",
      "Epoch 77, Loss: 5.779804229736328\n",
      "Epoch 78, Loss: 5.770045757293701\n",
      "Epoch 79, Loss: 5.760244846343994\n",
      "Epoch 80, Loss: 5.750419616699219\n",
      "Epoch 81, Loss: 5.740595817565918\n",
      "Epoch 82, Loss: 5.730772972106934\n",
      "Epoch 83, Loss: 5.720935344696045\n",
      "Epoch 84, Loss: 5.711078643798828\n",
      "Epoch 85, Loss: 5.701211452484131\n",
      "Epoch 86, Loss: 5.691348552703857\n",
      "Epoch 87, Loss: 5.68148946762085\n",
      "Epoch 88, Loss: 5.671624660491943\n",
      "Epoch 89, Loss: 5.661746978759766\n",
      "Epoch 90, Loss: 5.65186071395874\n",
      "Epoch 91, Loss: 5.641982555389404\n",
      "Epoch 92, Loss: 5.632113933563232\n",
      "Epoch 93, Loss: 5.622238636016846\n",
      "Epoch 94, Loss: 5.612346172332764\n",
      "Epoch 95, Loss: 5.602444648742676\n",
      "Epoch 96, Loss: 5.592546463012695\n",
      "Epoch 97, Loss: 5.582653045654297\n",
      "Epoch 98, Loss: 5.572747707366943\n",
      "Epoch 99, Loss: 5.562827110290527\n",
      "Epoch 100, Loss: 5.552885055541992\n",
      "Epoch 101, Loss: 5.5429277420043945\n",
      "Epoch 102, Loss: 5.5329508781433105\n",
      "Epoch 103, Loss: 5.522950172424316\n",
      "Epoch 104, Loss: 5.512922286987305\n",
      "Epoch 105, Loss: 5.502864360809326\n",
      "Epoch 106, Loss: 5.492774486541748\n",
      "Epoch 107, Loss: 5.4826507568359375\n",
      "Epoch 108, Loss: 5.472494125366211\n",
      "Epoch 109, Loss: 5.46229887008667\n",
      "Epoch 110, Loss: 5.452054500579834\n",
      "Epoch 111, Loss: 5.441757678985596\n",
      "Epoch 112, Loss: 5.431412220001221\n",
      "Epoch 113, Loss: 5.421016216278076\n",
      "Epoch 114, Loss: 5.41055965423584\n",
      "Epoch 115, Loss: 5.4000372886657715\n",
      "Epoch 116, Loss: 5.389445781707764\n",
      "Epoch 117, Loss: 5.378791332244873\n",
      "Epoch 118, Loss: 5.368066787719727\n",
      "Epoch 119, Loss: 5.357262134552002\n",
      "Epoch 120, Loss: 5.34637451171875\n",
      "Epoch 121, Loss: 5.335408687591553\n",
      "Epoch 122, Loss: 5.324357032775879\n",
      "Epoch 123, Loss: 5.313214302062988\n",
      "Epoch 124, Loss: 5.301975727081299\n",
      "Epoch 125, Loss: 5.290638446807861\n",
      "Epoch 126, Loss: 5.279200553894043\n",
      "Epoch 127, Loss: 5.267656326293945\n",
      "Epoch 128, Loss: 5.256001949310303\n",
      "Epoch 129, Loss: 5.244232654571533\n",
      "Epoch 130, Loss: 5.232346057891846\n",
      "Epoch 131, Loss: 5.220338821411133\n",
      "Epoch 132, Loss: 5.208203315734863\n",
      "Epoch 133, Loss: 5.195937633514404\n",
      "Epoch 134, Loss: 5.183536529541016\n",
      "Epoch 135, Loss: 5.170993804931641\n",
      "Epoch 136, Loss: 5.1583051681518555\n",
      "Epoch 137, Loss: 5.145468235015869\n",
      "Epoch 138, Loss: 5.1324782371521\n",
      "Epoch 139, Loss: 5.119330883026123\n",
      "Epoch 140, Loss: 5.1060261726379395\n",
      "Epoch 141, Loss: 5.092560768127441\n",
      "Epoch 142, Loss: 5.078936576843262\n",
      "Epoch 143, Loss: 5.065152645111084\n",
      "Epoch 144, Loss: 5.051215171813965\n",
      "Epoch 145, Loss: 5.037121772766113\n",
      "Epoch 146, Loss: 5.022877216339111\n",
      "Epoch 147, Loss: 5.008484363555908\n",
      "Epoch 148, Loss: 4.993953704833984\n",
      "Epoch 149, Loss: 4.97929048538208\n",
      "Epoch 150, Loss: 4.964504718780518\n",
      "Epoch 151, Loss: 4.949605464935303\n",
      "Epoch 152, Loss: 4.934597492218018\n",
      "Epoch 153, Loss: 4.919485569000244\n",
      "Epoch 154, Loss: 4.90427827835083\n",
      "Epoch 155, Loss: 4.888978481292725\n",
      "Epoch 156, Loss: 4.873594760894775\n",
      "Epoch 157, Loss: 4.858133316040039\n",
      "Epoch 158, Loss: 4.842597484588623\n",
      "Epoch 159, Loss: 4.826992988586426\n",
      "Epoch 160, Loss: 4.811324119567871\n",
      "Epoch 161, Loss: 4.795596599578857\n",
      "Epoch 162, Loss: 4.779813289642334\n",
      "Epoch 163, Loss: 4.763976573944092\n",
      "Epoch 164, Loss: 4.748092174530029\n",
      "Epoch 165, Loss: 4.7321624755859375\n",
      "Epoch 166, Loss: 4.71619176864624\n",
      "Epoch 167, Loss: 4.700180530548096\n",
      "Epoch 168, Loss: 4.6841301918029785\n",
      "Epoch 169, Loss: 4.6680402755737305\n",
      "Epoch 170, Loss: 4.651916027069092\n",
      "Epoch 171, Loss: 4.635758399963379\n",
      "Epoch 172, Loss: 4.61956787109375\n",
      "Epoch 173, Loss: 4.603348255157471\n",
      "Epoch 174, Loss: 4.587100028991699\n",
      "Epoch 175, Loss: 4.570827484130859\n",
      "Epoch 176, Loss: 4.554530143737793\n",
      "Epoch 177, Loss: 4.538212776184082\n",
      "Epoch 178, Loss: 4.521873950958252\n",
      "Epoch 179, Loss: 4.505518436431885\n",
      "Epoch 180, Loss: 4.489146709442139\n",
      "Epoch 181, Loss: 4.472760200500488\n",
      "Epoch 182, Loss: 4.45635986328125\n",
      "Epoch 183, Loss: 4.439946174621582\n",
      "Epoch 184, Loss: 4.423519611358643\n",
      "Epoch 185, Loss: 4.407080173492432\n",
      "Epoch 186, Loss: 4.390628814697266\n",
      "Epoch 187, Loss: 4.3741679191589355\n",
      "Epoch 188, Loss: 4.357694149017334\n",
      "Epoch 189, Loss: 4.341207027435303\n",
      "Epoch 190, Loss: 4.324709415435791\n",
      "Epoch 191, Loss: 4.308196067810059\n",
      "Epoch 192, Loss: 4.291671276092529\n",
      "Epoch 193, Loss: 4.275130748748779\n",
      "Epoch 194, Loss: 4.258578777313232\n",
      "Epoch 195, Loss: 4.242012023925781\n",
      "Epoch 196, Loss: 4.225432395935059\n",
      "Epoch 197, Loss: 4.208843231201172\n",
      "Epoch 198, Loss: 4.192244529724121\n",
      "Epoch 199, Loss: 4.175638675689697\n",
      "Epoch 200, Loss: 4.159030914306641\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.cpu(), y.cpu(), test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "model = NextWordBiLSTM(\n",
    "    embedding_matrix,\n",
    "    hidden_dim=128,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c18b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "def sample_top_k(output, top_k=5, temperature=0.8):\n",
    "\n",
    "    logits = output.squeeze() / temperature\n",
    "    probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "\n",
    "    top_indices = probs.argsort()[-top_k:]\n",
    "    top_probs = probs[top_indices]\n",
    "    top_probs = top_probs / top_probs.sum()  # نرمال‌سازی\n",
    "    chosen_idx = np.random.choice(top_indices, p=top_probs)\n",
    "    \n",
    "    return chosen_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a32fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تحلیل و شناسایی الگوهای رفتاری و این که و این و پویا به و این و این که تحلیل که و تحلیل و به و پویا به که تحلیل که و تحلیل و\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_text, next_words=50, seq_length=10,\n",
    "                  top_k=5, temperature=0.8, device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "    words = seed_text.split()\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # استفاده از آخرین seq_length کلمه\n",
    "        input_seq = [word2idx.get(w, word2idx[\"<unk>\"]) for w in words[-seq_length:]]\n",
    "        \n",
    "        # اگر seed کوتاه بود، با <pad> پر می‌کنیم\n",
    "        if len(input_seq) < seq_length:\n",
    "            input_seq = [word2idx[\"<pad>\"]] * (seq_length - len(input_seq)) + input_seq\n",
    "        \n",
    "        input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            predicted_idx = sample_top_k(output, top_k=top_k, temperature=temperature)\n",
    "            predicted_word = idx2word.get(predicted_idx, \"<unk>\")\n",
    "\n",
    "        if len(words) > 1 and predicted_word == words[-1]:\n",
    "            continue  \n",
    "        \n",
    "        words.append(predicted_word)\n",
    "    \n",
    "    # ✅ Return the generated text\n",
    "    return \" \".join(words)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "seed = \"تحلیل و شناسایی الگوهای رفتاری\"\n",
    "text = generate_text(model, seed_text=seed, next_words=30, \n",
    "                     seq_length=5, top_k=5, temperature=0.8, device=device)\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
